# -*- coding: utf-8 -*-
"""RESEARCH_Diffusion_Project_Generate_Egyptian_Characters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t1Azo4PErojk_j9U2Wof3OP-YV55-u9K

#### Links to Download

Egyptian Extended PDF: https://www.unicode.org/charts/PDF/U13460.pdf

NotoSansEgyptianHieroglyphs-Regular Font: https://fonts.google.com/noto/specimen/Noto+Sans+Egyptian+Hieroglyphs

### Step 1: Generate a Egyptian glyph and definition dataset
We can take the kDefinition text file to extract both characters and labels in the form of definitions.

https://github.com/amankiitg/Foundation_AI/blob/de84eb29f8e19b7a66ca8428f38fb818490cfad9/egypt.txt
"""

import pandas as pd
import re

#Initialize lists
unicodes = []
characters = []
descriptions = []
extras = []

file_path = "egypt.txt"
with open(file_path, 'r', encoding='utf-8') as file:
    lines = file.readlines()

i = 0
while i < len(lines):
    line = lines[i].strip()
    if re.match(r'^[0-9A-F]+\s+\S+\s+Egyptian Hieroglyph', line):
        # Main line with code, character, and description
        match = re.match(r'^([0-9A-F]+)\s+(\S+)\s+(Egyptian Hieroglyph .+)', line)
        if match:
            unicode_val = match.group(1)
            char = match.group(2)
            description = match.group(3)

            # Check next line for "‚Ä¢" comment
            extra = ""
            if i + 1 < len(lines) and '‚Ä¢' in lines[i + 1]:
                extra_line = lines[i + 1].strip()
                extra = re.sub(r'^‚Ä¢\s*', '', extra_line)
                i += 1  # Skip extra line

            unicodes.append(unicode_val)
            characters.append(char)
            descriptions.append(description)
            extras.append(extra)
    i += 1

#Create DataFrame
df = pd.DataFrame({
    'Unicode': unicodes,
    'Character': characters,
    'Description': descriptions,
    'Extra': extras
})

df.head()

import os

def count_files(directory_path):
  """Counts the number of files in a given directory.

  Args:
    directory_path: The path to the directory.

  Returns:
    The number of files in the directory.
  """
  if not os.path.isdir(directory_path):
    raise ValueError(f"'{directory_path}' is not a valid directory.")

  file_count = 0
  for item in os.listdir(directory_path):
    item_path = os.path.join(directory_path, item)
    if os.path.isfile(item_path):
      file_count += 1
  return file_count

# Example usage:
directory_path = "data/" # Replace with your directory path

num_files = count_files(directory_path)
print(f"The number of files in '{directory_path}' is: {num_files}")

"""### Step 2: Create the dataset of images

(1) Take a DataFrame df (like the one you previously created from Unihan-kDefinition.txt)

(2) Extract Unicode characters and render them as images using the specified font style (e.g., 128x128 PNGs).

(3) Save them into a directory using their Unicode code points as filenames.
"""

import pandas as pd
import os
from PIL import Image, ImageDraw, ImageFont

def create_image(character, font, image_size=(128, 128)):
    """
    Create an image of a single Unicode character.
    """
    image = Image.new('RGB', image_size, 'white')
    draw = ImageDraw.Draw(image)
    text_width, text_height = draw.textbbox((0, 0), character, font=font)[2:]
    x = (image_size[0] - text_width) / 2
    y = (image_size[1] - text_height) / 2 - 16
    draw.text((x, y), character, fill='black', font=font)
    return image

def is_character_supported(character, font, image_size=(128, 128)):
    """
    Check if the character is supported by the font.
    """
    image_char = create_image(character, font, image_size)
    image_unknown = create_image("ÔøΩ", font, image_size)  # U+FFFD is the replacement character

    return not image_char.tobytes() == image_unknown.tobytes()

def generate_and_save_images_from_df(df, font_path, column_name="Unicode", directory="data"):
    """
    Generate and save images for each character specified in the DataFrame's column.
    """
    if not os.path.exists(directory):
        os.makedirs(directory)

    font_size = 100
    font = ImageFont.truetype(font_path, font_size)

    for unicode_str in df[column_name]:
        code_point = int(unicode_str, 16)  # Convert hex string to integer
        character = chr(code_point)
        if is_character_supported(character, font):
            img = create_image(character, font)
            filename = os.path.join(directory, f"{code_point:04X}.png")
            img.save(filename)

# Assuming 'df' is your DataFrame and it is already defined and loaded
font_path = 'NotoSansEgyptianHieroglyphs-Regular.ttf'
# Update the function call accordingly
generate_and_save_images_from_df(df, font_path, column_name="Unicode", directory="data")

import os

def count_files(directory_path):
  """Counts the number of files in a given directory.

  Args:
    directory_path: The path to the directory.

  Returns:
    The number of files in the directory.
  """
  if not os.path.isdir(directory_path):
    raise ValueError(f"'{directory_path}' is not a valid directory.")

  file_count = 0
  for item in os.listdir(directory_path):
    item_path = os.path.join(directory_path, item)
    if os.path.isfile(item_path):
      file_count += 1
  return file_count

# Example usage:
directory_path = "data/" # Replace with your directory path

num_files = count_files(directory_path)
print(f"The number of files in '{directory_path}' is: {num_files}")

"""### Step 2b Extract more images from Unicode PDF"""

!pip install pymupdf

import fitz  # PyMuPDF
from PIL import Image
import os
import numpy as np
import cv2

def extract_region_from_pdf(pdf_path, page_number, coords, dpi=300, save_path='output.png'):
    """
    Extract a region from a PDF page as a high-quality image.

    Args:
        pdf_path (str): Path to the PDF file.
        page_number (int): Page number (0-indexed).
        coords (tuple): (x0, y0, x1, y1) in points (1/72 inch).
        dpi (int): Resolution for rendering the PDF.
        save_path (str): Where to save the extracted image.

    Returns:
        PIL.Image.Image: Extracted image region.
    """
    output_dir = 'png_matrix'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created output directory: {output_dir}")

    # Open the PDF
    doc = fitz.open(pdf_path)
    page = doc.load_page(page_number)

    # Calculate zoom factor from DPI
    zoom = dpi / 72
    mat = fitz.Matrix(zoom, zoom)

    # Render the page to a high-res image
    pix = page.get_pixmap(matrix=mat, alpha=False)

    # Convert to PIL Image
    image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    # Scale coordinates for high DPI
    x0, y0, x1, y1 = [int(c * zoom) for c in coords]

    # Crop and save
    cropped = image.crop((x0, y0, x1, y1))
    cropped.save(output_dir+'/'+str(page_number)+save_path)

    return cropped

region_coords = (68, 93, 512, 726)  # x0, y0, x1, y1 in PDF points (1/72")
for p in range(2,16,2):
  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')

region_coords = (101, 93, 544, 726)  # x0, y0, x1, y1 in PDF points (1/72")
for p in [1]:
  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')

region_coords = (100, 93, 544, 726)  # x0, y0, x1, y1 in PDF points (1/72")
for p in range(3,16,2):
  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')

import os
import pandas as pd
import numpy as np
from PIL import Image

def extract_grid_images(df, i, input_image_path, output_folder, grid_size=(16, 16), trim_px=(1, 1, 1, 0)):
    """
    Extract images from a grid and save them as separate files with boundary trimming.

    Args:
        input_image_path (str): Path to the input image containing the grid
        output_folder (str): Folder where extracted images will be saved
        grid_size (tuple): Number of rows and columns in the grid (rows, cols)
        trim_px (tuple): Pixels to trim from each side (left, top, bottom, right)
    """
    # Create output directory if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
        print(f"Created output directory: {output_folder}")

    # Load the image
    print(f"Loading image from: {input_image_path}")
    img = Image.open(input_image_path)

    # Get image dimensions
    img_width, img_height = img.size

    # Calculate the size of each cell
    cell_width = img_width / grid_size[1]
    cell_height = img_height / grid_size[0]

    # print(f"Image dimensions: {img_width}x{img_height}")
    # print(f"Cell dimensions: {cell_width}x{cell_height}")

    # Extract and save each cell
    total_cells = grid_size[0] * grid_size[1]
    saved_count = 0

    for row in range(grid_size[0]):
        for col in range(grid_size[1]):
            # Calculate coordinates with boundary trimming
            left = col * cell_width + trim_px[0]
            upper = row * cell_height + trim_px[1]
            right = (col + 1) * cell_width - trim_px[3]
            lower = (row + 1) * cell_height - trim_px[2]

            # print((left, upper, right, lower))
            # Crop the cell
            cell = img.crop((left, upper, right, lower))

            # Generate filename with row and column info
            # Using hex codes from the image (13D60 + row*16 + col)
            hex_code = format(0x13D60 + row * 16 + col, 'X')
            filename = str(i)+str(hex_code)+".png"
            df.loc[len(df.index)] = [str(i)+str(hex_code), filename]
            filepath = os.path.join(output_folder, filename)

            # Save the cell
            cell = cell.resize((128, 128), Image.Resampling.LANCZOS)
            cell.save(filepath)
            saved_count += 1

            # Print progress
            # if saved_count % 16 == 0:
            #     print(f"Progress: {saved_count}/{total_cells} images saved")

    print(f"Extraction complete! {saved_count} images saved to {output_folder}")

df = pd.DataFrame(columns=['Unicode','filename'])
# for i in range(1,2):
#   input_image = "png_matrix/"+str(i)+"cropped_image.png"  # Path to your grid image
#   output_folder = "data"#+str(i)  # Output folder name
#   extract_grid_images(df, i, input_image, output_folder, trim_px=(3, 3, 30, 3))

# Full usage
for i in range(1,11):
  input_image = "png_matrix/"+str(i)+"cropped_image.png"  # Path to your grid image
  output_folder = "data"#+str(i)  # Output folder name
  extract_grid_images(df, i, input_image, output_folder, trim_px=(3, 3, 30, 3))

for i in range(11,16):
  input_image = "png_matrix/"+str(i)+"cropped_image.png"  # Path to your grid image
  output_folder = "data"#+str(i)  # Output folder name
  extract_grid_images(df, i, input_image, output_folder, grid_size=(16, 15), trim_px=(3, 3, 30, 3))

num_files = count_files(directory_path)
print(f"The number of files in '{directory_path}' is: {num_files}")

df.head()

"""### Step 3 Rotating Images to Expand Dataset"""

import os
from PIL import Image
from multiprocessing import Pool, cpu_count

# Configuration
input_folder = "data"
output_folder = os.path.join(input_folder)
os.makedirs(output_folder, exist_ok=True)
angles = range(-25, 26, 5)
extensions = ('.png')

# Function to rotate and crop to original size
def rotate_and_save(args):
    filepath, angle = args
    try:
        with Image.open(filepath) as img:
            img = img.convert("RGBA")
            original_size = img.size
            rotated = img.rotate(angle, resample=Image.BICUBIC, expand=True)
            cropped = rotated.crop((
                (rotated.width - original_size[0]) // 2,
                (rotated.height - original_size[1]) // 2,
                (rotated.width + original_size[0]) // 2,
                (rotated.height + original_size[1]) // 2
            ))
            base_name, ext = os.path.splitext(os.path.basename(filepath))
            save_path = os.path.join(output_folder, f"{base_name}_{angle}deg_rotated{ext}")
            cropped.save(save_path)
    except Exception as e:
        print(f"Error processing {filepath} at {angle} degrees: {e}")

# Prepare tasks
tasks = []
for filename in os.listdir(input_folder):
    if filename.lower().endswith(extensions):
        filepath = os.path.join(input_folder, filename)
        for angle in angles:
            tasks.append((filepath, angle))

# Run in parallel
if __name__ == '__main__':
    with Pool(cpu_count()) as pool:
        pool.map(rotate_and_save, tasks)
    print("All rotations complete.")

num_files = count_files(directory_path)
print(f"The number of files in '{directory_path}' is: {num_files}")

"""### Step 4: Define training model configuration"""

from dataclasses import dataclass

@dataclass
class TrainingConfig:
    image_size = 128  # assumes images are square
    train_batch_size = 32
    eval_batch_size = 32
    num_epochs = 5
    gradient_accumulation_steps = 1
    learning_rate = 1e-4
    lr_warmup_steps = 500
    save_image_epochs = 1
    save_model_epochs = 30
    # image_size               = 256
    # train_batch_size         = 8
    # eval_batch_size          = 8
    # num_epochs               = 10
    # gradient_accumulation_steps = 1
    # learning_rate            = 1e-4
    # lr_warmup_steps          = 500
    # save_image_epochs        = 1           # save
    # save_model_epochs        = 5           # checkpoints every 5 epochs
    mixed_precision = "fp16"  # `no` for float32, `fp16` for automatic mixed precision
    output_dir = "glyffuser-unconditional"  # the model name
    overwrite_output_dir = True  # overwrite the old model when re-running the notebook
    seed = 0
    dataset_name="data128"

config = TrainingConfig()

"""### Step 5: Create Local Dataset Class and helper functions"""

!pip install datasets
from datasets import Dataset
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
import torchvision.transforms as T
import torch.nn.functional as F
from diffusers import UNet2DModel, DDPMScheduler, DPMSolverMultistepScheduler
from diffusers.optimization import get_cosine_schedule_with_warmup
from diffusers import DDPMPipeline
from accelerate import Accelerator
import torch
from tqdm.auto import tqdm
import os

def normalize_neg_one_to_one(img):
    return img * 2 - 1

# UPDATED LocalDataset class
class LocalDataset(Dataset):
    def __init__(self, folder, image_size, exts=['png']):
        super().__init__()
        self.folder = folder
        self.image_size = image_size

        # Match extensions case-insensitively
        self.paths = []
        for ext in exts:
            self.paths.extend([
                p for p in Path(folder).rglob(f'*.{ext}')
                if p.suffix.lower() == f'.{ext.lower()}'
            ])

        assert len(self.paths) > 0, f"No images found in {folder}. Check path and extensions."

        self.transform = T.Compose([
            T.Resize((image_size, image_size)),
            T.ToTensor(),
            T.Lambda(normalize_neg_one_to_one),
        ])

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, index):
        path = self.paths[index]
        img = Image.open(path).convert('L')  # Grayscale
        return self.transform(img)

def make_grid(images, rows, cols):
    # Helper function for making a grid of images
    w, h = images[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    for i, image in enumerate(images):
        grid.paste(image, box=(i%cols*w, i//cols*h))
    return grid

def evaluate(config, epoch, pipeline):
    # Sample from the model and save the images in a grid
    images = pipeline(
        batch_size=config.eval_batch_size,
        generator=torch.Generator(device='cpu').manual_seed(config.seed),
        num_inference_steps=50
    ).images

    image_grid = make_grid(images, rows=4, cols=4)

    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")

"""### Step 6: Define model training loop"""

def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        project_dir=os.path.join(config.output_dir, "logs")
    )

    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        accelerator.init_trackers("train_example")

    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            clean_images = batch
            noise = torch.randn(clean_images.shape).to(clean_images.device)
            bs = clean_images.shape[0]
            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)
                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        if accelerator.is_main_process:
            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)
                save_dir = os.path.join(config.output_dir, f"epoch{epoch}")
                pipeline.save_pretrained(save_dir)

# # =====================================================================
# # ‚ùª Training loop ‚Äì checkpoints + metrics + plots
# # ---------------------------------------------------------------------

# def train_loop(cfg, model, noise_scheduler, optimizer, dataloader, lr_scheduler, start_epoch=0):
#     accelerator = Accelerator(mixed_precision=cfg.mixed_precision,
#                               gradient_accumulation_steps=cfg.gradient_accumulation_steps,
#                               log_with="tensorboard",
#                               project_dir=str(Path(cfg.output_dir) / "logs"))
#     if accelerator.is_main_process:
#         (Path(cfg.output_dir) / "samples").mkdir(parents=True, exist_ok=True)

#     # Add error handling and optimizer state restoration
#     if start_epoch > 0:
#         if cfg.resume_checkpoint:
#             try:
#                 pipe = DDPMPipeline.from_pretrained(cfg.resume_checkpoint)
#                 model.load_state_dict(pipe.unet.state_dict())
#                 noise_scheduler = pipe.scheduler

#                 # Optionally load optimizer state if saved
#                 optimizer_path = Path(cfg.resume_checkpoint) / "optimizer.pt"
#                 if optimizer_path.exists():
#                     optimizer.load_state_dict(torch.load(optimizer_path))

#                 print(f"üîÑ Resumed from {cfg.resume_checkpoint} (epoch {start_epoch})")
#             except Exception as e:
#                 print(f"‚ö†Ô∏è Failed to load checkpoint: {e}")
#                 print("Starting from scratch instead.")
#                 start_epoch = 0
#         else:
#             print(f"‚ö†Ô∏è start_epoch > 0 but no resume_checkpoint provided. Starting from epoch 0.")
#             start_epoch = 0

#     model, optimizer, dataloader, lr_scheduler = accelerator.prepare(model, optimizer, dataloader, lr_scheduler)

#     loss_hist, kl_hist = [], []
#     global_step, total_start = start_epoch*len(dataloader), time.time()

#     for epoch in range(start_epoch, cfg.num_epochs):
#         epoch_loss = 0.0
#         epoch_kl_sum = 0.0
#         epoch_start = time.time()
#         pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{cfg.num_epochs}", dynamic_ncols=True,
#                      disable=not accelerator.is_local_main_process)

#         for batch in pbar:
#             clean = batch; noise = torch.randn_like(clean)
#             t = torch.randint(0, noise_scheduler.num_train_timesteps, (clean.size(0),), device=clean.device).long()
#             noisy = noise_scheduler.add_noise(clean, noise, t)
#             with accelerator.accumulate(model):
#                 pred = model(noisy, t, return_dict=False)[0]
#                 loss = F.mse_loss(pred, noise)
#                 kl   = 0.5*loss
#                 accelerator.backward(loss)
#                 accelerator.clip_grad_norm_(model.parameters(), 1.0)
#                 optimizer.step(); lr_scheduler.step(); optimizer.zero_grad()

#             # loss bookkeeping
#             epoch_loss += loss.item()
#             epoch_kl_sum += kl.item()
#             kl_hist.append((global_step, kl.item()))

#             pbar.set_postfix(loss=f"{loss.item():.4f}", kl=f"{kl.item():.4f}")
#             accelerator.log({
#                 "loss": loss.item(),
#                 "kl": kl.item(),
#                 "lr": lr_scheduler.get_last_lr()[0]  # Added learning rate logging
#             }, step=global_step)
#             global_step += 1

#         # ---- epoch end ---- (properly indented outside the batch loop)
#         if accelerator.is_main_process:
#             mean_loss = epoch_loss / len(dataloader)
#             mean_kl   = epoch_kl_sum / len(dataloader)

#             loss_hist.append((epoch + 1, mean_loss))
#             epoch_time = time.time() - epoch_start
#             print(f"‚è±Ô∏è Epoch {epoch+1} finished in {epoch_time:.1f}s ‚Äì mean loss {mean_loss:.4f} | mean KL {mean_kl:.4f}")

#             # 1) sample grid every N epochs
#             if (epoch + 1) % cfg.save_image_epochs == 0:
#                 try:
#                     pipe = DDPMPipeline(unet=accelerator.unwrap_model(model),
#                                         scheduler=inference_scheduler)
#                     evaluate(cfg, epoch + 1, pipe)
#                 except Exception as e:
#                     print(f"‚ö†Ô∏è Error generating samples: {e}")

#             # 2) checkpoint every N epochs
#             if (epoch + 1) % cfg.save_model_epochs == 0 or (epoch + 1) == cfg.num_epochs:
#                 try:
#                     pipe = DDPMPipeline(unet=accelerator.unwrap_model(model),
#                                         scheduler=inference_scheduler)
#                     ckpt_dir = Path(cfg.output_dir) / f"epoch{epoch+1}"
#                     pipe.save_pretrained(ckpt_dir)

#                     # Save optimizer state
#                     torch.save(optimizer.state_dict(), ckpt_dir / "optimizer.pt")

#                     print(f"‚úÖ Saved checkpoint ‚Üí {ckpt_dir}")
#                 except Exception as e:
#                     print(f"‚ö†Ô∏è Error saving checkpoint: {e}")

#     # Training complete
#     if accelerator.is_main_process:
#         total_time = time.time() - total_start
#         print(f"‚úÖ Training complete in {total_time/60:.1f} min")

#         # Generate plots if we have data
#         try:
#             if loss_hist:
#                 # Loss plot
#                 ep, l = zip(*loss_hist)
#                 plt.figure(figsize=(6, 4))
#                 plt.plot(ep, l, marker='o')
#                 plt.xlabel('Epoch')
#                 plt.ylabel('Loss')
#                 plt.title('Training Loss')
#                 plt.grid(True)
#                 plt.savefig(Path(cfg.output_dir) / 'loss_curve.png')
#                 plt.close()  # Close figure to free memory

#                 # KL plot
#                 if kl_hist:
#                     st, kl = zip(*kl_hist)
#                     plt.figure(figsize=(6, 4))
#                     plt.plot(st, kl)
#                     plt.xlabel('Global step')
#                     plt.ylabel('KL')
#                     plt.title('KL divergence')
#                     plt.grid(True)
#                     plt.savefig(Path(cfg.output_dir) / 'kl_curve.png')
#                     plt.close()  # Close figure to free memory
#         except Exception as e:
#             print(f"‚ö†Ô∏è Error generating plots: {e}")

"""### Step 7: Load dataset"""

# Define data source
dataset = LocalDataset("data", image_size=config.image_size)
train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)

"""### Step 8: Define UNet Model"""

# Define model
model = UNet2DModel(
    sample_size=config.image_size,  # the target image resolution
    in_channels=1,  # the number of input channels
    out_channels=1,  # the number of output channels
    layers_per_block=1,  # how many ResNet layers to use per UNet block
    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block
    down_block_types=(
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",
        "AttnUpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

"""### Step 9: Define optimizers and schedulers"""

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
inference_scheduler = DPMSolverMultistepScheduler()
optimizer = AdamW(model.parameters(), lr=config.learning_rate)
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=config.lr_warmup_steps,
    num_training_steps=(len(train_dataloader) * config.num_epochs),
)

"""### Step 10: Run training on the model!"""

import pandas as pd
import os
from PIL import Image, ImageDraw, ImageFont

from accelerate import notebook_launcher
import time

args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)
# notebook_launcher(train_loop, args, num_processes=1)

"""### Step 11: Sample from trained diffusion model

Loads a pretrained DDPMPipeline from epoch99 of a local model directory (likely trained on glyphs or characters).

Moves the model to GPU.

Sets the scheduler to DPMSolverMultistepScheduler, which controls the denoising steps during sampling.

Generates 16 images in one batch.

Uses a fixed random seed (config.seed) for reproducibility.

Uses 50 diffusion steps to sample clean images from noise.

Arranges the 16 images into a 4x4 grid.

Creates a samples folder inside your config.output_dir.

Saves the resulting image grid as samples.png.
"""

model_path = "glyffuser-unconditional/epoch4"  # Path to the specific epoch model directory# Path to the model directory
pipeline = DDPMPipeline.from_pretrained(model_path).to("cuda")
pipeline.scheduler = DPMSolverMultistepScheduler()

# Sample from the model and save the images in a grid
images = pipeline(
    batch_size=16,
    generator=torch.Generator(device='cuda').manual_seed(config.seed), # Generator can be on GPU here
    num_inference_steps=50
).images

# Make a grid out of the inverted images
image_grid = make_grid(images, rows=4, cols=4)

# Save the images
test_dir = os.path.join(config.output_dir, "samples")
os.makedirs(test_dir, exist_ok=True)
image_grid.save(f"{test_dir}/samples.png")

"""### Step 12: Visualize model generated images"""

from diffusers import DPMSolverMultistepScheduler, DDPMPipeline
from PIL import Image, ImageDraw, ImageFont
import torch

def make_labeled_grid(images, prompt, steps, font_path=None, font_size=20, margin=10):
    assert len(images) == len(steps), "The number of images must match the number of steps"

    w, h = images[0].size
    font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()

    # Calculate the height of the grid including the margin for text
    total_height = h + margin + font_size
    total_width = w * len(images)
    grid_height = total_height + margin + font_size  # Add extra margin for the prompt
    grid = Image.new('RGB', size=(total_width, grid_height), color=(255, 255, 255))
    # Draw the text prompt at the top
    draw = ImageDraw.Draw(grid)
    prompt_text = f"Prompt: \"{prompt}\""
    prompt_width, prompt_height = draw.textbbox((0, 0), prompt_text, font=font)[2:4]
    prompt_x = (total_width - prompt_width) / 2
    prompt_y = margin / 2
    draw.text((prompt_x, prompt_y), prompt_text, fill="black", font=font)

    for i, (image, step) in enumerate(zip(images, steps)):
        # Calculate position to paste the image
        x = i * w
        y = margin + font_size

        # Paste the image
        grid.paste(image, box=(x, y))

        # Draw the step text
        step_text = f"Steps: {step}"
        text_width, text_height = draw.textbbox((0, 0), step_text, font=font)[2:4]
        text_x = x + (w - text_width) / 2
        text_y = y + h + margin / 2 - 8
        draw.text((text_x, text_y), step_text, fill="black", font=font)
    return grid

# Initialize the model pipeline using your local model
model_path = "glyffuser-unconditional/epoch4"  # Path to your trained model
pipeline = DDPMPipeline.from_pretrained(model_path).to("cuda")
pipeline.scheduler = DPMSolverMultistepScheduler()

# Define the number of steps to visualize
num_inference_steps_list = [1, 2, 3, 5, 10, 20, 50]

images = []

# Generate images for each value in num_inference_steps_list
for num_steps in num_inference_steps_list:
    generated_images = pipeline(
        batch_size=1,
        generator=torch.Generator(device='cuda').manual_seed(0),
        num_inference_steps=num_steps
    ).images
    images.append(generated_images[0])  # Append the generated image

# Create the labeled grid with a descriptive prompt since this is an unconditional model
prompt = "Unconditional Diffusion Model Output"
image_grid = make_labeled_grid(images, prompt, num_inference_steps_list)

# Show the grid
from IPython.display import display
display(image_grid)

# Save the image grid
image_grid.save("diffusion_steps_visualization30.png")

"""### Step 13: Create animated visualization"""

from diffusers import DPMSolverMultistepScheduler, DDPMPipeline
from PIL import Image, ImageDraw, ImageFont
import torch
import numpy as np
import os
import imageio.v2 as imageio
from tqdm import tqdm

# Initialize the model pipeline using your local model
model_path = "glyffuser-unconditional/epoch4"  # Path to your trained model
pipeline = DDPMPipeline.from_pretrained(model_path).to("cuda")
pipeline.scheduler = DPMSolverMultistepScheduler()

# Create output directory for frames
os.makedirs("animation_frames", exist_ok=True)

# Set parameters
num_inference_steps = 50
seed = 41

# Use the model's forward process to generate images at each step
print("Generating denoising frames...")

# Start with pure noise (t=1000)
generator = torch.Generator(device="cuda").manual_seed(seed)

# Store all frames
frames = []

# The correct way to visualize the denoising process is to use the pipeline with
# increasing numbers of denoising steps
for step in tqdm(range(0, num_inference_steps + 1, 2)):  # Skip some steps for faster generation
    if step == 0:
        # For the initial noise, just use the pipeline with 1 step
        # This will effectively show the noise
        current_step = 1
    else:
        current_step = step

    # Generate the image with the current number of denoising steps
    image = pipeline(
        batch_size=1,
        generator=torch.Generator(device="cuda").manual_seed(seed),
        num_inference_steps=current_step
    ).images[0]

    # Save the frame
    image.save(os.path.join("animation_frames", f"frame_{step:03d}.png"))
    frames.append(image)

# Create GIF from frames
print("Creating GIF animation...")
# Ensure all frames have the same size (shouldn't be necessary but just in case)
frames_resized = [frame.resize((256, 256)) for frame in frames]

# Save as GIF
output_gif = "diffusion_process5.gif"
frames_resized[0].save(
    output_gif,
    save_all=True,
    append_images=frames_resized[1:],
    optimize=False,
    duration=150,  # milliseconds per frame - slower to see the changes
    loop=0  # 0 means loop indefinitely
)

print(f"Animation saved to {output_gif}")

# Create a grid showing selected frames
def create_process_grid(frames, num_to_show=8):
    # Select frames evenly throughout the process
    if len(frames) <= num_to_show:
        selected_frames = frames
    else:
        indices = np.linspace(0, len(frames)-1, num_to_show, dtype=int)
        selected_frames = [frames[i] for i in indices]

    # Resize frames
    width, height = 256, 256
    selected_frames = [frame.resize((width, height)) for frame in selected_frames]

    # Create grid image
    cols = min(4, num_to_show)
    rows = (num_to_show + cols - 1) // cols

    grid = Image.new('RGB', (width * cols, height * rows))

    for i, frame in enumerate(selected_frames):
        row = i // cols
        col = i % cols
        grid.paste(frame, (col * width, row * height))

    return grid

# Create and save the grid
grid = create_process_grid(frames)
grid.save("diffusion_process_grid5.png")
print("Process grid saved to diffusion_process_grid5.png")

"""# Wrapper for research experiment"""

# Create a grid showing selected frames
def create_process_grid(frames, num_to_show=8):
    # Select frames evenly throughout the process
    if len(frames) <= num_to_show:
        selected_frames = frames
    else:
        indices = np.linspace(0, len(frames)-1, num_to_show, dtype=int)
        selected_frames = [frames[i] for i in indices]

    # Resize frames
    width, height = 256, 256
    selected_frames = [frame.resize((width, height)) for frame in selected_frames]

    # Create grid image
    cols = min(4, num_to_show)
    rows = (num_to_show + cols - 1) // cols

    grid = Image.new('RGB', (width * cols, height * rows))

    for i, frame in enumerate(selected_frames):
        row = i // cols
        col = i % cols
        grid.paste(frame, (col * width, row * height))

    return grid

def make_labeled_grid(images, prompt, steps, font_path=None, font_size=20, margin=10):
    assert len(images) == len(steps), "The number of images must match the number of steps"

    w, h = images[0].size
    font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()

    # Calculate the height of the grid including the margin for text
    total_height = h + margin + font_size
    total_width = w * len(images)
    grid_height = total_height + margin + font_size  # Add extra margin for the prompt
    grid = Image.new('RGB', size=(total_width, grid_height), color=(255, 255, 255))
    # Draw the text prompt at the top
    draw = ImageDraw.Draw(grid)
    prompt_text = f"Prompt: \"{prompt}\""
    prompt_width, prompt_height = draw.textbbox((0, 0), prompt_text, font=font)[2:4]
    prompt_x = (total_width - prompt_width) / 2
    prompt_y = margin / 2
    draw.text((prompt_x, prompt_y), prompt_text, fill="black", font=font)

    for i, (image, step) in enumerate(zip(images, steps)):
        # Calculate position to paste the image
        x = i * w
        y = margin + font_size

        # Paste the image
        grid.paste(image, box=(x, y))

        # Draw the step text
        step_text = f"Steps: {step}"
        text_width, text_height = draw.textbbox((0, 0), step_text, font=font)[2:4]
        text_x = x + (w - text_width) / 2
        text_y = y + h + margin / 2 - 8
        draw.text((text_x, text_y), step_text, fill="black", font=font)
    return grid

import shutil
from google.colab import files

from dataclasses import dataclass

@dataclass
class TrainingConfig:
    image_size = 128  # assumes images are square
    train_batch_size = 32
    eval_batch_size = 32
    num_epochs = 10 #Only 10 epochs
    gradient_accumulation_steps = 1
    learning_rate = 1e-4
    lr_warmup_steps = 500
    save_image_epochs = 1
    save_model_epochs = 10
    mixed_precision = "fp16"  # `no` for float32, `fp16` for automatic mixed precision
    output_dir = None  # the model name
    overwrite_output_dir = True  # overwrite the old model when re-running the notebook
    seed = 0
    dataset_name="data128"

config = TrainingConfig()


def run_and_save_experiment(
    exp_name,
    test_name,
    config,
    model,
    noise_scheduler,
    optimizer,
    dataloader,
    lr_scheduler,
    train_loop,
    make_labeled_grid,
    create_process_grid,
    num_processes=1,
    font_path=None
):
    # Set output path for this experiment/test combo
    config.output_dir = os.path.join("experiments", exp_name, test_name)
    os.makedirs(config.output_dir, exist_ok=True)

    # Train model
    args = (config, model, noise_scheduler, optimizer, dataloader, lr_scheduler)
    notebook_launcher(train_loop, args, num_processes=num_processes)

    # Load model pipeline from final saved epoch
    model_dir = os.path.join(config.output_dir, f"epoch{config.num_epochs - 1}")
    pipeline = DDPMPipeline.from_pretrained(model_dir).to("cuda")
    pipeline.scheduler = DPMSolverMultistepScheduler()

    print("üåÄ Generating image samples from trained model...")
    images = pipeline(
        batch_size=16,
        generator=torch.Generator(device='cuda').manual_seed(config.seed),
        num_inference_steps=50
    ).images

    # Save sample grid
    base_path = config.output_dir
    grid = make_grid(images, rows=4, cols=4)
    grid.save(os.path.join(base_path, "samples.png"))

    # Labeled grid from step-wise generations
    print("üåÄ Creating labeled grid...")
    num_inference_steps_list = [1, 2, 3, 5, 10, 20, 50]
    stepwise_images = []

    for steps in num_inference_steps_list:
        result = pipeline(
            batch_size=1,
            generator=torch.Generator(device='cuda').manual_seed(config.seed),
            num_inference_steps=steps
        ).images
        stepwise_images.append(result[0])

    labeled_grid = make_labeled_grid(
        stepwise_images,
        prompt="Unconditional Diffusion Model Output",
        steps=num_inference_steps_list,
        font_path=font_path
    )
    labeled_grid.save(os.path.join(base_path, "labeled_grid.png"))

    # Generate GIF from denoising steps
    print("üåÄ Creating diffusion process GIF and grid...")
    frames = []
    for step in range(0, 51, 2):
        current_step = step if step > 0 else 1
        img = pipeline(
            batch_size=1,
            generator=torch.Generator(device='cuda').manual_seed(config.seed),
            num_inference_steps=current_step
        ).images[0]
        frames.append(img)

    frames[0].save(
        os.path.join(base_path, "diffusion_process.gif"),
        save_all=True,
        append_images=frames[1:],
        duration=150,
        loop=0
    )

    process_grid = create_process_grid(frames)
    process_grid.save(os.path.join(base_path, "process_grid.png"))

    # # Create ZIP of the output directory
    # zip_path = f"{config.output_dir}_{test_name}.zip"
    # print(f"üì¶ Zipping results to {zip_path}...")
    # shutil.make_archive(base_name=zip_path.replace(".zip", ""), format="zip", root_dir=config.output_dir)

    # # Automatically download the zip file
    # files.download(zip_path)
    # Zip only files in the main output directory (skip subfolders)
    zip_filename = f"{config.output_dir}.zip"
    with zipfile.ZipFile(zip_filename, 'w') as zipf:
        for file in os.listdir(config.output_dir):
            filepath = os.path.join(config.output_dir, file)
            if os.path.isfile(filepath):
                zipf.write(filepath, arcname=file)

    # Trigger download of the zip file
    files.download(zip_filename)

    print(f"‚úÖ Completed experiment: {exp_name}/{test_name}")

"""# Research Minimum Dataset"""

import os
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import re

class EgyptianCharacterDataset(Dataset):
    def __init__(self, image_dir, angle_range=None, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_paths = self._select_images(angle_range)

    def _select_images(self, angle_range):
        images = os.listdir(self.image_dir)
        if angle_range is None:
            return [os.path.join(self.image_dir, img) for img in images if not re.search(r"_[-]?\d+deg_rotated", img)]
        else:
            angles = set(f"{angle}deg_rotated" for angle in angle_range)
            return [
                os.path.join(self.image_dir, img)
                for img in images
                if any(f"_{angle}" in img for angle in angles)
            ]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
      # Change to convert to grayscale ('L') instead of RGB ('RGB')
      image = Image.open(self.image_paths[idx]).convert("L")
      if self.transform:
          image = self.transform(image)
      return image

# Define transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    # Add normalization to match the LocalDataset transformation
    # Assuming LocalDataset normalized to [-1, 1]
    # This is achieved by ToTensor (0-1) followed by lambda x: x * 2 - 1
    transforms.Lambda(lambda x: x * 2 - 1)
])

# Variants
angles_none = None
angles_small = list(range(-5, 6, 5))
angles_large = list(range(-15, 16, 5))

dataset_none = EgyptianCharacterDataset("data", angle_range=angles_none, transform=transform)
dataset_small = EgyptianCharacterDataset("data", angle_range=angles_small, transform=transform)
dataset_large = EgyptianCharacterDataset("data", angle_range=angles_large, transform=transform)

dataloader_none = DataLoader(dataset_none, batch_size=config.train_batch_size, shuffle=True)
dataloader_small = DataLoader(dataset_small, batch_size=config.train_batch_size, shuffle=True)
dataloader_large = DataLoader(dataset_large, batch_size=config.train_batch_size, shuffle=True)

for test_name, dl in {
    "dataset_none": dataloader_none,
    "dataset_small": dataloader_small,
    "dataset_large": dataloader_large,
}.items():
    print(f"Starting experiment: {test_name}")

    run_and_save_experiment(
        exp_name="exp1",
        test_name=test_name,
        config=config,
        model=model,
        noise_scheduler=noise_scheduler,
        optimizer=optimizer,
        dataloader=dl,
        lr_scheduler=lr_scheduler,
        train_loop=train_loop,
        make_labeled_grid=make_labeled_grid,
        create_process_grid=create_process_grid,
        font_path="NotoSansEgyptianHieroglyphs-Regular.ttf"  # Or any available path
    )

"""# Research with model hyperparameters"""

learning_rates = [1e-4, 5e-4, 1e-3]
epoch_counts = [10, 25, 50]  # Note: still only loading from `epoch9`

for use_attention in [True, False]:
    model_type = "with_attention" if use_attention else "no_attention"

    for lr in learning_rates:
        for ep in epoch_counts:
            print(f"\nüîß Running Experiment: {model_type}, LR={lr}, Epochs={ep}")

            config = TrainingConfig()
            config.learning_rate = lr
            config.num_epochs = ep
            config.output_dir = f"exp3/{model_type}_lr{lr}_ep{ep}"

            model = get_model(with_attention=use_attention)

            # Assume training was already done and model saved in epoch9
            run_and_save_experiment(config, model, dataloader, test_name=f"{model_type}_lr{lr}_ep{ep}")

"""# Scheduler"""

from diffusers import DDPMScheduler, DDIMScheduler
from transformers import get_scheduler
import torch

# Fixed configuration
default_lr = 1e-4
default_epochs = 10
default_with_attention = True

# Variants to test
scheduler_variants = [
    ("DDPMScheduler", DDPMScheduler),
    ("DDIMScheduler", DDIMScheduler),
]

lr_scheduler_types = [
    "constant",
    "cosine",
    "linear",
]

# Load model once for each experiment ‚Äî inference only
for noise_sched_name, noise_sched_class in scheduler_variants:
    for lr_sched_type in lr_scheduler_types:
        print(f"\nüß™ Experiment: {noise_sched_name} + LR Scheduler: {lr_sched_type}")

        # Setup training config
        config = TrainingConfig()
        config.learning_rate = default_lr
        config.num_epochs = default_epochs
        config.output_dir = f"exp4_scheduler_test/{noise_sched_name}_{lr_sched_type}"

        # Load model saved from epoch 9 of prior training
        model_dir = os.path.join(config.output_dir, "epoch9")
        pipeline = DDPMPipeline.from_pretrained(model_dir)

        # Setup scheduler
        noise_scheduler = noise_sched_class(num_train_timesteps=1000)

        dummy_optimizer = torch.optim.Adam(pipeline.unet.parameters(), lr=config.learning_rate)
        total_steps = len(dataloader) * config.num_epochs
        lr_scheduler = get_scheduler(
            name=lr_sched_type,
            optimizer=dummy_optimizer,
            num_warmup_steps=config.lr_warmup_steps,
            num_training_steps=total_steps,
        )

        # Evaluate using the experiment-specific noise and lr schedulers
        run_and_save_experiment(
            config=config,
            model=pipeline.unet,
            dataloader=dataloader,
            test_name=f"{noise_sched_name}_{lr_sched_type}",
            noise_scheduler=noise_scheduler,
            lr_scheduler=lr_scheduler,
        )

"""# Research with Unet model architecture"""

model_without_attention = UNet2DModel(
    sample_size=config.image_size,
    in_channels=1,
    out_channels=1,
    layers_per_block=1,
    block_out_channels=(128, 128, 256, 256, 512, 512),
    down_block_types=(
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",  # removed attention
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",
        "UpBlock2D",  # removed attention
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

config.output_dir = "exp3/unet_no_attention"
run_and_save_experiment(config, model_without_attention, dataloader=your_dataloader, test_name="no_attention")

from diffusers import UNet2DModel

model_with_attention = UNet2DModel(
    sample_size=config.image_size,
    in_channels=1,
    out_channels=1,
    layers_per_block=1,
    block_out_channels=(128, 128, 256, 256, 512, 512),
    down_block_types=(
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",
        "AttnUpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

config.output_dir = "exp3/unet_attention"
run_and_save_experiment(config, model_with_attention, dataloader=your_dataloader, test_name="with_attention")